下面我们把第 3.2 节中出现的 “函数 B” 和 “Split Tree” 这两个核心概念逐条拆解，力求把它们在整体算法里到底起什么作用、需要什么样的数据结构、以及背后的设计动机都说得更清楚、更透彻。

---

## 一、为什么需要两个新函数（类似于排序时的 b 和 π）

在一维排序问题里，我们定义了两个函数：

1. **$b(\cdot)$**：把一个组内的数值 $(x_{i_1},\ldots,x_{i_m})$ 映射到“它们各自属于哪个桶（bucket）”——也就是在预先构造好的 V-list （分位点）里，对每个 $x_{i_j}$ 找到它落在哪个区间 $[v_r,v_{r+1})$ 中。
2. **$\pi(\cdot)$**：又把同一组里这些数值“按相对大小”压缩成一个 Lehmer 码，便于我们在线性（加熵）时间内恢复组内排序顺序。

在二维 Delaunay 三角化 (DT) 问题里，我们也要做同样的“分段”＋“局部结构检索”工作，只不过：

* “分段” 的意思对应的是：\*\*给定一个常数点集 $V\subset\mathbb{R}^2$，它的 Delaunay 三角形 (Del(V)) 已经事先算好了。现在当局部插入新的点 $p_i\in\mathbb{R}^2$ 时，我们要首先知道 **这个点落在哪个三角形内部**，然后才能决定后续怎么把它和周围的点连边插入。
* “局部结构检索” 的意思对应的是：我们要在某个叫 “Split Tree（拆分树）” 的空间分层结构里，快速找到适合用来“增量构造” Delaunay 三角化的那些局部节点（例如做最新点的邻域查找、或做局部重构时要访问的三角形列表）。

所以作者在 3.2 节先后引入了：

1. **函数 $B$** ——它就像一维排序里的 $b$，是一个“点定位（point‐location）”函数，用来把新插入的点 $p_i$ 映射到“它位于哪个 Delaunay 三角形 $t_r\in \mathrm{Del}(V)$ 里”。
2. **函数 $\mathrm{SplitT}(\cdot)$** ——它就像一维排序里的 $\pi$，是一个“将整张点集按某种层次规则切分成许多框架（split rectangles），并在树中存下每个节点对应的子集和外包矩形”的结构。这个“Split Tree” 后续可以用来把 Delaunay 算法做成“每次处理一个节点相关的小块”来实现近熵的增量插入，最终保证总期望时间是线性。

下面我们一条条看这两个函数的定义、作用以及需要用到的数据结构。

---

## 二、函数 $B$：点定位到 Delaunay 三角形

### 2.1 定义和形式化说明

先看原文给出的描述：

> **“Let $|\mathrm{Del}(V)|$ denote the number of triangles in $\mathrm{Del}(V)$.  Let the triangles in $\mathrm{Del}(V)$ be denoted by $t_1,\ldots,t_{|\mathrm{Del}(V)|}$.  For every input instance $\;I=(p_1,\ldots,p_n)$, the first function $B$ returns the triangles in $\mathrm{Del}(V)$ that contain the $p_i$ʼs.**
>
> $$
>   B: \;\bigcup_{m=1}^n \bigl(\,\mathbb{R}^{2m}\bigr)\;\longrightarrow\;\bigcup_{m=1}^n \bigl(\,[\,1,|\mathrm{Del}(V)|\,]^m\bigr),
>   \quad
>   \text{such that for every \(m\)-tuple of points }(q_1,\dots,q_m), 
>   \;B(q_1,\dots,q_m) = (\,r_1,\dots,r_m),
> $$
>
> 其中 $r_i$ 就是 $\mathrm{Del}(V)$ 里恰好“包含”点 $q_i$ 的三角形编号 $\in[1,|\mathrm{Del}(V)|]$。

用更口语一点的话来说：

1. **输入**：当我们要对一个新实例 $I=(p_1,\dots,p_n)$ 做 Delaunay 插入时，需要知道这 $n$ 个点“在原始的三角化 $\mathrm{Del}(V)$ 中哪个三角形里”。
2. **输出**：函数 $B$ 会返回一个长度为 $n$ 的整型序列 $(r_1,\dots,r_n)$，其中每个 $r_i\in\{1,2,\dots,|\mathrm{Del}(V)|\}$。如果 $p_i$ 落在三角形 $t_{r_i}$ 内部（或者在它的边上），我们就说“$r_i$ 是 $p_i$ 在 $\mathrm{Del}(V)$ 里的三角形索引”。

* 当 $m=1$ 时，$B$ 只需要找一个点在哪个三角形里；
* 当 $m>1$ 时，我们把这些点一起传给 $B$，它一次性返回一个 $\,m$-元组 $(r_1,\dots,r_m)$。

这是\*\*典型的“平面点定位（planar point location）”\*\*问题：给定一个已经存在的三角化（或者一般的分割），如何快速回答“一个查询点 $q$ 落在哪个三角形／哪个单元里？”

---

### 2.2 函数 $B$ 在算法中为什么必须高效

在后面整个 Delaunay 插入的流程里，我们会反复插入新点到当前的 Delaunay 结构（基础结构是“$\mathrm{Del}(V)$”）之中。每次插入一个点 $p$，都要先找到它在已知三角化中的“定位三角形”。如果这一定位用最简单的“对每条三角形边做扫线判断”那类蛮力方法，期望完成一次点定位就可能需要 $O(\log N)$ 或更坏的时间。而我们真正要的，是对 $n$ 个新点做查询 **总时间越接近 $O(n)$** 越好（其中常数最好和整体熵（输入分布的复杂度）挂钩）。这也是论文想把“自适应排序里的熵编码思路”搬到 Delaunay 问题里来：

* 在排序里，函数 $b$ 的设计是“先把数值用 V-list 切桶，然后做一个 Trie+near‐optimal BST 的检索结构”，以 $O(H(\,b(\cdot)\,)+m)$ 的平均查询时间把一个长度为 $m$ 的子序列里所有桶号都查完。
* 在 Delaunay 里，函数 $B$ 就是把“$\mathrm{Del}(V)$ 的三角形索引”当做“桶号”，我们要为它做一个等价的高效检索结构，使得对每次新查询点 $p$ 都能在「接近点定位信息熵」$+O(1)$ 的期望时间里，准确地返回它所在的三角形编号。

换句话说，**实现 $B$ 所需的数据结构就是一个“面分割点定位（Planar Subdivision Point‐Location）”的熵编码检索系统**，它在训练阶段知道整个 Delaunay 三角化 $\mathrm{Del}(V)$（也就是知道每个三角形和它的邻居结构），并可以根据所有可能输入点 $p$ 的分布估计每个三角形被请求的频率，然后按照“near‐optimal BST + Trie” 的思路来组织。这样在运行阶段，只要用近熵的时间就能找到“$p$ 在哪个三角形里”。

> **需要的数据结构（Point‐Location Structure）**
>
> 1. **静态三角化存储（$\mathrm{Del}(V)$）**：
>
>    * 你需要把整个 $\mathrm{Del}(V)$ 分成一个“平面子割” (planar subdivision)，例如用“DCEL（Doubly Connected Edge List）”或者 “每个三角形存它三个顶点的坐标加它到相邻三角形的索引”，方便 O(1) 地枚举三角形边和邻居。
> 2. **熵编码的点定位检索**：
>
>    * 类似于排序里给定 $m$ 个桶然后做的 Trie + nearly‐optimal BST。这里我们要把「$\mathrm{Del}(V)$ 中的所有三角形」看作“离散输出集合” $\{\beta_1,\dots,\beta_M\}$，其中 $M=|\mathrm{Del}(V)|$。
>    * 在训练阶段，用足够多的样本点（这些样本点均匀分布或挖掘自真实应用分布），统计每个三角形 $t_j$ 被调用（被查询 “哪个三角形包含样本点”）的频率 $\hat p_j$。
>    * 然后和一维排序的做法一模一样：
>
>      1. 把各个三角形编号 $\beta_j$ 做一个 Trie（或称前缀树），其实就是一棵把所有可能“点所在三角形路径”串联在一起的树；
>      2. 在 Trie 的每个内部节点 $u$，把它所有的子边（代表“可能的下一层空间分割”）放到一个 nearly‐optimal BST $A_u$ 里，权重就是“下一步切换到某个孩子” 的概率；
>      3. 这样运行阶段对每个查询点 $p$，从三角化的根开始，一边“沿着Trie走 + 在对应 BST 里查找具体的哪条边””来确认下一步进入哪个三角形，直到找到最终叶子（三角形编号）。这个过程的平均时间约 $O(\log(1/\hat p_j))$，加上路径长度。
>    * 如果一个点位置从未在训练样本里出现过，那 Trie 会“早早失败”，算法就退化到传统的 $O(\log n)$ 算法（比如直接在 DCEL 上做二分搜索或跳跃搜索）；但这部分退化的概率在训练样本足够多的情况下可忽略，总体成功率仍然 $1-O(1/n)$。

> **总结**：函数 $B$ 所依赖的就是
>
> * 一份 **静态的** $\mathrm{Del}(V)$ 三角形列表、DCEL 或其它面分割存储；
> * 一棵 **Trie+near‐optimal BST** 组合而成的点定位“熵编码检索树”，保证对随机查询点 $p$ 以近熵时间 $\bigl(O(H(p\mapsto t_j)) +O(1)\bigr)$ 找到它所处的三角形 $t_j$。

---

## 三、函数 $\mathrm{SplitT}(\cdot)$：Fair Split Tree（拆分树）

### 3.1 为什么要把点集做拆分树？

当我们在二维做 Delaunay 三角化时，如果直接用「随机增量插入＋翻边 (flip)」之类的算法，总体期望可能是 $O(n\log n)$ 或略低，但要达到 $O(n)$ 甚至 $O(n + H_S)$ 的目标，需要一个更细致的分治或空间分层策略。论文中所用的经典套路是：

1. **先把 {$n$ 个待插入点} 组织成一个“Fair Split Tree”**，它是一棵二叉树，每个节点存放这一子树里所有点的**最小外包矩形** $R(w)$ 与一个更“大一些”的**外包正方形** $\widehat{R}(w)$。
2. 这棵拆分树保证 **“子矩形之间要么相互分离得很远，要么极度接近地重叠”**（即所谓的 “well‐separated pair decomposition” (WSPD) 原语）。
3. 利用 WSPD 的性质，可以把 Delaunay 的构造分成“若干个足够分离的点对集合”上的局部重构，这些局部重构各自代价很小。最后把它们拼起来，就得到全局的 Delaunay Triangulation，且总体期望时间 $O(n)$。

在 3.2 节中，我们首先看到了一种特殊的 Fair Split Tree——叫做 **Halving Split Tree（对半拆分树）**，它的定义非常规整：

> **“Halving Split Tree of $P$”**
>
> 1. **根节点** $v$：
>
>    * $R(v)$ 是包含所有 $P$ 的最小轴对齐矩形；
>    * $\widehat{R}(v)$ 是以 $R(v)$ 的中心为中心、且边长等于 $\max\{\text{长宽}\}$ 的最小正方形；
>    * 根节点里存放点集 $P$。
> 2. 如果某个节点 $w$ 存放的不止一个点：
>
>    * 首先找出 $R(w)$（它就是这一子集的最小轴对齐包围矩形）；
>    * 在 $R(w)$ 的 **最长边** 上做“对半切分”，这条“切分线”与最长边垂直。
>
>      * 将 $P\cap R(w)$ 里的点分成两份：左（或上）一半 vs 右（或下）一半，保证两份非空；
>      * 同时，把 $\widehat{R}(w)$ 也沿这条切分线拆成对应两块，分别赋给两个子节点的 $\widehat{R}(\cdot)$。
>    * 这就在 $w$ 下生成了两个孩子节点 $\ell$ 和 $r$，每个节点里存放各自的子集，以及对应的 $R(\ell), \widehat{R}(\ell)$ 和 $R(r), \widehat{R}(r)$。
> 3. 不断 **递归**：对每个子节点再做同样的 “最长边对半切分” 直到子节点只剩 1 个点为止，这时它成为叶子。
> 4. 因此最终的树是一棵 **满二叉树**，有 $|P|$ 片叶子，每个叶子正好对应一颗单点，内节点对应若干点的“拆分集合”。

这棵 “Halving Split Tree” 有两个重要的几何性质：

* **每一层拆分都保证左右两子集非空**。逼近于“每次把矩形的一条最长边二等分、然后把点分到二者” 的做法，保证树高 $O(\log |P|)$，并且形状比较“平衡”。
* **所有节点的 “外包正方形” $\widehat{R}(u)$ 满足一个良好的比例关系**，即

  $$
    \ell_{\min}\bigl(\widehat{R}(u)\bigr) \;\ge\; \frac13 \;\ell_{\max}\bigl(R(\mathrm{parent}(u))\bigr).
    \tag{6}
  $$

  其中 $\ell_{\min}(\cdot)$ 表示矩形最短边长，$\ell_{\max}(\cdot)$ 表示最长边长。这个不等式保证了：

  1. 子节点的包围正方形不会“比父节点乘以太多系数”后才包含父包围矩形。
  2. 从而可以用来证明，每对“节点间要做交互的两个点集 (P\_u,P\_v)”要么“互相远离”，要么“非常接近”，给出一个 **well-separated pair decomposition**（WSPD）。

在真正做 Delaunay 三角化的“自适应算法”里，我们并不一定严格按照“最长边对半切”来拆分。我们只要满足 **Fair Split Tree** 的定义即可：

* **Fair Split Tree** 的定义比“Halving Split Tree”更宽松——它允许“切分位置”不一定恰好是最长边的正中间，只要切分线“足够接近中间、保持左右孩子集大小平衡、并维持（6）式的比例关系”就行。
* 论文引用了 Arya–Mount–etc. \[3] 中的结果：任何满足“每个节点的包围外正方形 $\widehat{R}(u)$ 与父节点的长边满足 $\ell_{\min}(\widehat{R}(u))\ge\frac13\,\ell_{\max}(R(\mathrm{parent}(u)))$”的拆分，都可以保证最终从这棵树里导出一个线性大小的 WSPD。

**因此**，在第 3.2 节我们看到：

> “We use $\mathrm{SplitT}(P)$ to denote a fair split tree of $P$, not necessarily the halving split tree.  Using (6), it can be shown that a fair split tree can be used to produce a well-separated pair decomposition of $O(|P|)$ size \[3], which can then be used to produce a DT in $O(|P|)$ expected time as we explain in Section 3.3 later.”

换句话说，**SpliltT$P$ 只是给出了一种“把点集 $P$ 做成满足 (6) 条件的二叉拆分树”**，后面我们要重点用它来：

1. 在训练阶段，根据分布推断“哪些切分线放在哪里”更有可能把点分得比较平均，同时也满足 (6) 的几何比例。
2. 在运行阶段，把新来的 $n$ 个点分到这棵树里，利用它内部的 WSPD 信息，能用 **期望 $O(|P|)$** 的时间完成 Delaunay Triangulation。

---

### 3.2 详细拆解定义与属性

首先，我们再把“Halving Split Tree”那段话拆得更细、看清楚每一句都是什么意思——并且顺便把“Fair Split Tree（公平拆分树）”的差别说清楚。

#### 步骤 1：根节点初始化

* **点集**：令 $P\subset\mathbb{R}^2$ 是当前待构造 Delaunay 的所有点。
* **最小外包矩形** $R$：它是按坐标轴对齐的最小长方形，能够完整包裹所有点 $P$。

  * 找法：扫描点集 $P$ 取 $\min_x P,\max_x P,\min_y P,\max_y P$ 四个数，就能确定一个最小包围长方形 $R$。
  * 编程实现：一次遍历所有点 $O(|P|)$ 时间就能得到。
* **最小包围正方形** $\widehat{R}$：以 $R$ 的中心为中心、边长等于 $\max\{\text{(长方形 \(R$ 的宽度)},\text{(长方形 $R$ 的高度)}}) 的正方形。它保证“长方形 $R$ 完全落在正方形 $\widehat{R}$ 内”。

  * 例如如果 $R$ 本身宽度比高度大，那么 $\widehat{R}$ 的边长就等于 $R$ 的宽度，高度补齐到同样长度。
* **在树上建一个根节点 $v$**：

  * 令「$v$.`R` = $R$；
  * 令「$v$.`\widehat R` = $\widehat{R}$；
  * 令「$v$.`point_set` = $P$」。
  * 这是拆分树的顶层，当前存放了所有点，还没做任何拆分。

#### 步骤 2：递归拆分每个节点 $w$

只要某个节点 $w$ 存放的点集 “$|P_w|>1$”（不止一个点），我们就要把它拆分成两个孩子。具体做法是：

1. **取子集点集**： $P_w = \{\,p\in P: p \text{ 存在于 }w\}$。

2. **计算它的最小外包矩形 $R(w)$**：

   * 与“根节点”类似，扫描这个节点里已有的所有点，计算最小 $\min_x,\max_x,\min_y,\max_y$，构成矩形 $R(w)$。

3. **计算它的最小包围正方形 $\widehat{R}(w)$**：

   * 将上一条得到的 $R(w)$ 的中心作为正方形中心，边长取 $\max\{\text{矩形宽},\text{矩形高}\}$，构造一个正方形。
   * 保证 $\widehat{R}(w)$ 里包住整个 $R(w)$。

4. **决定切分方向**：

   * 看 $R(w)$ 的长/宽哪个更大，也就是 $\ell_{\max}(R(w))=\max\{\text{宽度},\text{高度}\}$。
   * 取那条最长边所在方向，在那条边百份之五十（中间）处画一条垂直线——这条线就把矩形 $R(w)$ 分成“左/右”或“上/下”两块相等宽度的子矩形，保证每边宽度一半。

5. **把点集分成两部分**：

   * 那条线把所有点 $P_w$ 切成两份：左/右或者上/下。因为我们在这条“最长边中点处”下切，一般能保证“左右两边都至少有一个点”（因为我们只做这个拆分当且仅当 $|P_w|>1$）。
   * 将落在切线左侧的点统统归到左孩子；落在切线右侧的点归到右孩子。
   * 编程实现时，只要遍历一次 $P_w$ 里的所有点，比较它们的 $x$（或 $y$）坐标与切分线坐标，就能分到两支。

6. **构造两个子节点**：

   * 左孩子 `w.left`：存放 $P_{w\_L}=\{\,p\in P_w: p$ 在切线左侧$\}$，它的包围框令 `R(w.left)` = 包含 $P_{w\_L}$ 的最小外包矩形；
     令 `widehatR(w.left)` = 最小包围正方形（边长与父节点同或更小，但中心对齐）。
   * 右孩子 `w.right`：同理，存放 $P_{w\_R}=\{\,p\in P_w: p$ 在切线右侧$\}$，构造 `R(w.right)` 和 `widehatR(w.right)`。
   * 保证 “左子点集、右子点集都非空”。

7. **递归**：对 `w.left`、`w.right` 都重复 “如果点集大小 > 1，就继续拆分” 的步骤，直到叶子节点只剩 1 个点为止。

完成之后，这棵树就变成了一棵 **满二叉树**，树的深度大约 $O(\log |P|)$，因为每次我们都是沿最长维度“对半切分”，大致保证了“子集规模”平衡。

---

### 3.3 Fair Split Tree vs Halving Split Tree

* 上面描述的严格“最长边对半切”那种叫 **Halving Split Tree**（对半拆分树）。它天然就是一种 **公平拆分 (fair split)**，因为每次都把 $R(w)$ 直接等分，而且子矩形与父节点的比例关系可以严格写出 $\ell_{\min}(\widehat R(w))=\frac12\,\ell_{\max}(R(w))$，也满足式 (6) 的条件。

* 论文里反复强调的 **Fair Split Tree** 则“允许更灵活一点”：

  1. 不一定要切到$R(w)$最长边的**绝对中点**，只要“足够靠近中间”、能保证左右两边点都不为空，且保证最小外包正方形 $\widehat R(\cdot)$ 与父节点满足 (6) 中的“$\ell_{\min}(\widehat{R}(u)) \ge \tfrac13\,\ell_{\max}(R(\mathrm{parent}(u)))$” 就行。
  2. 换言之，“Fair Split” 只要求“外包正方形不至于过于扭曲” (参见式 (6))，但对“长方形 $R(\cdot)$ 拆分时的位置”并不强制。只要切分线大致在“中间区域”切，就可以把我们想要保持的“树高度 $\,O(\log|P|)$”和“WSPD 大小 $O(|P|)$” 的性质保留住。

> **式 (6)：$\ell_{\min}(\widehat R(u)) \;\ge\;\tfrac13\,\ell_{\max}(R(\mathrm{parent}(u)))$.**
>
> * 令 $\ell_{\min}(\cdot)$ 表示“矩形最短边长”，$\ell_{\max}(\cdot)$ 表示“矩形最长边长”。
> * 当你把一个节点$u$ 从父节点 p 拆分出来后，由于“取的是 $\mathrm{parent}(p)$ 最长边对半切” 或者“Fair Split 里略微偏离中间但不至于太偏”，最后产生的外包正方形 $\widehat R(u)$ 所有边都与 $\ell_{\max}(R(p))$ 保持一个常数级别的下界关系（至少 $\frac13$ 倍）。
> * 这正是【Arya–Mount–etc.】里证明 WSPD 可控大小的关键。他们在 Lemma 4.1 里指出，只要满足 (6)，最终导出的 WSPD size 是 $O(|P|)$，且“分离度” (separation ratio) 不会太差。

总结来说：

* **Halving Split Tree**：每次都完全用 “最长边的中点” 去对半拆，严格对半分割，配出来的树高度最好、结构最规整，一定是 Fair Split Tree 的一个特例。
* **Fair Split Tree**：可以在“中点附近任取”切分位置，只要求满足 (6) 就行。相对更灵活，但理论依然能保证导出一个 $O(|P|)$ 大小的 WSPD。

在实际算法中往往“先随机绘制一个大致的 Fair Split 剖分” 或 “用采样估计哪条切分线更合适”来满足熵编码需求。最终输出的就是 $\mathrm{SplitT}(P)$，后文会把它作为“每个子问题局部 Delaunay Triangulation”与“局部点定位”结合的基础。

---

## 四、如何用这些结构来加速 Delaunay 三角化

到这里，我们已经把 3.2 节搞清了两个核心部件：

1. **函数 $B$** ——把一个查询点定位到原始三角网 $\mathrm{Del}(V)$ 的具体三角形里。
2. **函数 $\mathrm{SplitT}(P)$** ——把一组点 $P$ 做成一棵 “满足 (6) 条件、能导出线性大小 WSPD 的 Fair Split Tree”。

下面简单概述一下它们在整个 Delaunay 算法里**接下来的角色**，以便你对它们的“最终目的是啥”有一个整体印象（尽管具体的“合并 WSPD→DT”细节要在 3.3 节再展开）。

---

### 4.1 训练阶段／构造阶段的思路

1. **先构造“基准三角网” $\mathrm{Del}(V)$**

   * 这里的 $V\subset\mathbb{R}^2$ 通常是“训练阶段我们从真实分布里采的一大堆点”或者“我们预先固定好的某个点集”。
   * 我们把它的 Delaunay Triangulation 事先算出来，得到一组三角形列表 $\{\,t_1,\dots,t_M\}$，以及它们的邻接信息（方便后续任何点定位）。
   * 这一步相当于为“函数 $B$”准备好了“所有可能的三角形输出集合”的定义域。

2. **为函数 $B$ 构建熵编码点定位数据结构**

   * 用大量样本点 $\{q^{(1)},q^{(2)},\dots\}$ 在 $\mathrm{Del}(V)$ 上做“落点测试”，统计每一个三角形 $t_j$ 被随机查询点落入的相对频率 $\widehat p_j$。
   * 按照排序里“Trie + nearly‐optimal BST” 的套路，把所有三角形索引 $\{1,\dots,M\}$ 用训练频率做一次熵编码，得到一棵 “点定位 Trie” $T_B$。
   * 这样一来，当运行阶段要定位一个新查询点 $p\sim\mathcal D$ 时，只需沿树 $T_B$ 走到叶子，就可以在 “近熵时间 $O(\log(1/\widehat p_j))$” 里得出“它落在三角形 $t_j$” 的结论。

3. **为函数 $\mathrm{SplitT}(\cdot)$ 构建熵编码拆分规则**

   * 同样地，我们要根据真实输入分布，估计“平均下来哪些节点的哪条切分线更可能会分到‘某些点子集’”。
   * 也就是说，把“可能出现的拆分树形状”视作输出候选集合 $\{\beta_1,\dots,\beta_{\tau}\}$，把训练样本里落在“某个位置”产生的拆分决策做频率统计，然后做 Trie＋near‐optimal BST 熵编码。
   * 最终得到一棵“可以近熵地、在线地从上到下一步步确定拆分线”的熵编码拆分树，让我们在运行阶段把刚到的点 $(p_1,\dots,p_n)$ 快速分摊到各个 $\mathrm{SplitT}(\cdot)$ 的节点中，形成一棵 Fair Split Tree。

> **注意**：实际上训练阶段 $\mathrm{SplitT}$ 的构造要复杂得多，牵涉到“代数几何”里判断“哪些点应该归同一个节点 / 拆到哪条切分线上”之类的精巧手段。后文 Lemma 3.4 会详细给出“如何从 $O(n^3)$ 个样本中几乎肯定地恢复一个满足 Definition 3.3 条件的近似分区”，它是搭建$\mathrm{SplitT}$ 的关键。但从宏观上看——**我们需要一个“可以近熵地在线回答：‘当前节点要用哪条切分线’”的熵编码数据结构**，它所用的正是“Trie+near‐optimal BST” 的套路。

---

### 4.2 运行阶段：用 $B$ 和 $\mathrm{SplitT}(\cdot)$ 做最快的 Delaunay

当我们进入真正的运行阶段（operation phase），有一个新的实例 $I=(p_1,\dots,p_n)$，我们要做的流程大致如下（后面 3.3 节会展开）：

1. **将所有新点分到 $\mathrm{SplitT}(I)$ 的节点里**

   * 从根开始，查找“当前节点该用哪条切分线？”，利用熵编码的“二维拆分 Trie”快速回答；把 $p_i$ 投进去后决定它到左子树还是右子树；
   * 继续递归，直到把所有新点的索引放到某一片“叶子节点”里。因为 $\mathrm{SplitT}(I)$ 满足“(6)”的分离比，我们可以确保“某些远离的子树里点不需要互相做 Delaunay 关系”，从而把整个 $I$ 分成若干个“局部可并行做 DT” 的子问题。
   * 这部分的**期望开销**就是“沿拆分树走的深度”$+$“在拆分节点里查找去哪个孩子”的熵编码时间，总体接近 $O(n + H(\mathrm{Split决策}))$。

2. **在每个叶子节点做小规模 Delaunay**

   * 叶子里剩下来的若干点，大多数是在某个很小的包围矩形里（因为拆分树越来越往下），它们之间彼此“互相影响”但和其他叶子几乎 没有影响。
   * 我们可以在每个叶子上的点集里局部做 Delaunay（三角化或局部翻边），因为点数非常少，甚至只剩 1 个或 2 个，就不需要再分治。
   * 这部分等同于“$\pi$”在排序里负责“组内排序”的作用。

3. **按 WSPD（由拆分树推出来）合并各局部 Delaunay**

   * 拆分树的性质保证我们能线性大小地列出“哪些子节点需要相互做点对 Delaunay 检查”——这就是 well‐separated pair decomposition (WSPD)。
   * 对于每一对“在 WSPD 里被列到一起”的两组点，只需做 $O(1)$ 次“相邻三角形”翻边检测，就能把它们之间的边正确插入。
   * 而 WSPD 总大小是 $O(n)$，所以整场合并工作也只消耗 $O(n)$ 的期望时间。

最终流程保证了：**函数$B$ + “拆分树$\mathrm{SplitT}(\cdot)$” + WSPD 合并** 这三部分，能够让我们在训练分布已知、且用足够样本构造好熵编码结构的前提下，用 **期望 $O(n)$** 时间就完成新的点集 $I$ 的 Delaunay 三角化。

---

## 五、必备的各个数据结构一览

下面把在这一节（3.2）里实际“要做”或者“要预先构造”的全部数据结构罗一个大列表，顺便标注它们各自的用途：

| 数据结构名称                                                      | 放在哪里  | 存储的信息 | 作用／为什么需要它 |
| ----------------------------------------------------------- | ----- | ----- | --------- |
| **静态 Delaunay Triangulation**<br>($\mathrm{Del}(V)$ + DCEL) | 训练结束后 |       |           |

1. 顶点坐标、
2. 三角形顶点列表（每个三角形 $t_j$ 的 3 个顶点）
3. 邻接关系（每个三角形的 3 条边对应的相邻三角形索引）  |

* 用于构造函数 $B$：点定位时，知道“我现在在三角形 $t_j$ 里”、“要沿哪个边看下一个三角形”；
* 可以做常见的“沿边走”操作（point‐in‐triangle test + 边邻接跳转）。
  |
  \| **熵编码点定位 Trie+BST**<br>(对应函数 $B$)           | 训练阶段构造 |

1. Trie 树节点：代表“当前已定位到某层哪些三角形”？
2. 每个内部节点 $u$ 搭配一个 nearly‐optimal BST $A_u$，BST 的 key 是“下一步可能的三角形编号”
3. 每个 BST child 边存一个标签（对应三角形索引）和概率权重 |

* 训练时统计“在训练集里，点落到三角形 $t_j$ 的频率 $\hat p_j$”；
* 把所有三角形编号熵编码进 Trie 里，让运行阶段对给定点 $p$ 可以从根→叶 “近熵”地走下来；
* 目的是让点定位从 $O(\log |Del(V)|)$ 退化到 $O(H(p\mapsto t_j)+1)$ 的期望。
  |
  \| **（训）点分布采样表**                   | 训练阶段构造 |
* 记录足够多的“样本点在哪个三角形里”，
* 以及对应三角形编号出现次数 $\hat N_j$、频率 $\hat p_j=\hat N_j/N$。 |
* 用于计算 trie 里 BST 节点的权重：$\mathrm{weight}(t_j)=\hat p_j$。
  |
  \| **拆分树节点存储 (SplitT 架构)**      | 训练结束后   |

1. 每个节点 $u$ 存放它所包含的点子集 $P_u$ 的外包矩形 $R(u)$ ；
2. 外包正方形 $\widehat{R}(u)$；
3. 以及链接到“下一层切分决定”的指针（如果是熵编码版本，就会存“在当前节点该用哪条切分线？下一个在哪棵子树里？”的接入点）  |

* 训练阶段要用 Lemma 3.4 的方法判断“哪些输入点属于同一个近似子组”，把结果**汇总到这棵拆分树里**；
* 运行阶段，新的点 $p_i$ 到来时，直接沿这棵树从根→叶 “逐层决策” 应该分到左子树还是右子树；决定依据从训练阶段熵编码好的“当前节点哪些切分线更常发生”或“近半切的几何规则”。
  |
  \| **熵编码拆分决策 Trie+BST**           | 训练阶段构造 |

1. 的 Trie 树节点：代表“到当前拆分树里某一层，我们已有部分拆分条件(切分线) 选好”；
2. 每个内部节点 $u$ 对应一种“拆分线选择”的 BST $A_u$，BST key 是“所有可能的下一步切分线编号”；
3. 这些切分线编号有一个**训练频率**（多少个样本集里第 $i$ 次拆分是这条线），用于构造 near‐optimal BST 的权重。 |

* 运行阶段，当我们要将新点 $p$ 从根节点沿 “拆分树结构”推进时，就要询问“在节点 $u$ 上，给定已知的信息（父节点的包围矩形等），新点更有可能被哪条切分线划到左 / 右”？这时在 $A_u$ 上做一次近熵查找就得到“选 $l$” 或 “选 $r$”，大幅度压缩了分支决策的平均成本。
  |
  \| **WSPD (Well‐Separated Pair Decomposition)** | 运行阶段 |
* 一对一列出“拆分树哪两个不相邻的子树要做互相 Delaunay 检查”，
* 保证这样的“需要交互的子集对”总数 $O(|P|)$。 |
* 后续用来在每个相互靠近的节点对之间“做局部翻边检测／增量插入”，构建最终的 Delaunay Triangulation。
  |

---

## 六、总结：这一部分解析的核心要点

1. **函数 $B$ (Point Location)**

   * 作用：把一个查询点快速定位到一张已知三角网 $\mathrm{Del}(V)$ 中它所落的三角形编号；
   * 要用到的数据结构：

     1. 一份 **静态的 Delaunay Triangulation**（以 DCEL 或三角形邻接的方式存储）；
     2. 一个 **熵编码的 Point‐Location Trie+near‐optimal BST**，在运行阶段对每个点以“近熵时间”完成定位。

2. **函数 $\mathrm{SplitT}(\cdot)$ (Fair Split Tree / WSPD 构造)**

   * 作用：给定一个点集 $P$，构造一棵满足“每个节点外包正方形大小不低于父节点最长边的 $\tfrac13$” 的拆分树，用它来导出线性大小 WSPD，从而在后续阶段以线性期望时间完成 Delaunay；
   * 需要的数据结构：

     1. **拆分树节点存储**：每个节点记它当前所含点集的最小包围矩形 $R(u)$ 和最小包围正方形 $\widehat R(u)$，以及其父子指针；
     2. **熵编码拆分决策 Trie+near‐optimal BST**：训练阶段用来统计“在节点 $u$ 上最优的切分线位置／方向” 的出现频率，运行阶段可在近熵时间内一步步确定“到底往左子树切，还是往右子树切”。

3. **两者的结合如何加速算法**

   * 在训练阶段，分别基于排序思路做统计：

     * 用大量样本算出“每个三角形被查询的概率 $\hat p_j$” → 构造 Trie\_B；
     * 用大量样本算出“每个节点可选切分线的概率” → 构造 Trie\_split；
   * 在运行阶段，**插入 $n$ 个新点时**：

     1. 对于每个点，先用 $B$ 近熵定位三角形 → 便于后续“局部翻边插入”；
     2. 同时并行把这些点“抛进”拆分树 $\mathrm{SplitT}(I)$：沿树用尽可能接近中点／高概率的切分线把点分给左右孩子；
     3. 在叶子或拆分子树里做局部小规模 Delaunay；
     4. 最后用 WSPD（由拆分树保证大小 $O(n)$）做**线性**的“子树间合并检测/翻边”，完成完整的 Delaunay Triangulation。

> **为什么这一整套流程能做到 $O(n)$ 期望？**
>
> * **点定位 $B$**：均摊到 $n$ 个查询点，总成本接近 $\sum_i O(\log(1/\hat p_{r_i}) + 1) = O(n + H(\,p_i\mapsto t_{r_i}\,))$。
> * **拆分树构造 $\mathrm{SplitT}$**：均摊到 $n$ 个点总成本 $O(n + H(\text{拆分决策}))$。
> * **WSPD 合并**：因为 WSPD 大小 $O(n)$，所以所有局部翻边的总检查次数 $O(n)$。

这正是排序部分“ $O(n + H_S)$ ”的思想在二维 Delaunay 上的对应：

* 其中 “$B$” 对应“$b$”，负责把点落在哪个三角形里；
* “$\mathrm{SplitT}$” 对应“$\pi$”，负责“把点打到哪个拆分树子问题里”；
* WSPD 对应“桶内合并”，负责把所有局部结果拼成一个完整的全局结果。

到此，我们就完成了对 3.2 节“Functions for Point Location and Retrieval of Split Trees” 的详细剖析：

1. 明确了**为什么要引入**这两个函数
2. 给出了它们的**形式定义**
3. 讲清楚它们各自**需要哪些底层数据结构**
4. 概述了它们**在后续 Delaunay 构造中到底用来做什么**

接下来，如果你希望更深入地看到“在 C++/Python 里如何写出熵编码的 Trie+BST”或“如何用线性时间从 $\mathrm{SplitT}$ 导出 WSPD”，可以在 3.3 节及附录里找到更具体的伪代码和代数几何论证。但从整体架构上看， 3.2 节的重点就在于搭建这两个“点定位”＋“拆分树” 的桥梁，它们分别对应着排序里“分桶”与“组内排序”这两大步骤。

---

下面我们详细拆解并讲解论文中 **Theorem 3.9**（即把排序中的 Theorem 2.9 推广到“函数 $B$” 和 “函数 $\Pi$”）的内容。这个定理是实现“自适应 Delaunay 三角化”算法的关键工具：它保证我们能够用 **近熵** 时间（即期望 $O(H(\,f(I|_J)\,)+|J|)$）构造并查询数据结构，从而在线高效地得到 “点定位” 与 “拆分树” 的输出。本节将分成三大块来讲：

1. **定理陈述的含义**：把 Theorem 3.9 中的每一句都拆解开来看，它到底在说什么，特别是与排序里的 Theorem 2.9 有哪些对应与差异。
2. **Proof 的 Case 1：$f=B$** ——也就是如何为“点定位函数 $B$” 建立一个近熵查询的数据结构，它在训练和运行阶段究竟做了哪些事情。
3. **Proof 的 Case 2：$f=\Pi$** ——如何为“拆分树检索函数 $\Pi$” 构造近熵查询的数据结构。由于 $\Pi$ 的输出更复杂（不仅要输出一对排列顺序，还要输出一个拆分树形），这部分会更冗长，也更需要逐步剖析。

在正式进入 Case 1/Case 2 前，我们先把 Theorem 3.9 自身拆开，看看它要表达的核心意思是什么。

---

## 一、Theorem 3.9 的陈述与含义

> **Theorem 3.9 (论文原文翻译)**
> 令 $J\subseteq[1,n]$ 是某个固定的下标子集，且设 $\lvert J\rvert = m$。
> 令 $f$ 表示“函数 $B$” 或“函数 $\Pi$” 中的任意一个。记
>
> $$
>   N \;=\; \bigl\lceil t_0 \ln t_0 \ln n \bigr\rceil, 
>   \quad 
>   \text{其中 } t_0 = \mathrm{poly}(n)\text{ 是一个确定的多项式上界，用来束缚}\;\bigl|\{\,f(I|_J)\colon I\sim \mathcal D\}\bigr|.
> $$
>
> 那么我们可以构建一个数据结构，使得：
>
> 1. **训练时** （构造该数据结构）只需要 $\widetilde O\bigl(N\,|J|^2\bigr)$ 时间，并且要用到的 **训练样本数** 正好是 $N$（也就是需要从分布 $\mathcal D$ 独立抽 $I_1,\dots,I_N$）。
> 2. 该数据结构在 **占用空间** 上是 $O\bigl(t_0\,\lvert J\rvert\bigr)$。
> 3. **在运行时**（给定一个新的实例 $I$，我们只取其“$\;I|_J$ ”（即子集 $\{\,p_i: i\in J\}$）做查询），这个数据结构能够 **返回 $f(I|_J)$**，并且以概率 $1 - t_0^{-2}$ 保证“查询耗时的 **期望** 是 $O\bigl(H_J(f) + \lvert J\rvert \bigr)$”。
>    其中 $H_J(f)$ 是“对函数 $f(I|_J)$ 在分布 $\mathcal D$ 下的熵”。

通俗地说，**Theorem 3.9** 的意义是：

* 第一步，我们假设“所有可能的输入子集 $I|_J$”在分布 $\mathcal D$ 下通过 $f$ 所产生的**输出种类数**（$\lvert \{\,f(I|_J) : I\sim \mathcal D\}\rvert$）不超过某个多项式 $t_0 = \mathrm{poly}(n)$。（在前面 Lemma 3.8 中我们已经分别对 “函数 $B$” 和 “函数 $\Pi$” 证明了它们的输出种类数分别是 $O(n^2\,m^2)$ 或 $O(m^8)$，显然都被某个多项式 $t_0$ 所束缚。）
* 只要这个输出种类被多项式 $t_0$ 所控制，我们就可以仿照排排序里的“Trie + near‐optimal BST”思路，用 $N = O\bigl(t_0 \ln t_0 \ln n\bigr)$ 个训练样本独立采样来**近熵地**学习 $f$。
* 结果：构造数据结构的成本是 $\widetilde O\bigl(N\,m^2\bigr)$，查询数据结构（给定新的 $I$ 只要看 $I|_J$）的**期望**时间是 $O\bigl(H_J(f) + m\bigr)$。
* 其中熵 $H_J(f)$ 表示“随机采 $I\sim\mathcal D$ 后， $f(I|_J)$ 这件事本身所包含的信息量”（更常见的称呼：**信息熵**）。如果 $f(I|_J)$ 很“容易预测”（概率分布高度集中），那么 $H_J(f)$ 就很小；查询时间就主要是 $O(m)$ 的线性加上一个很小的熵加项。
* $\widetilde O(\cdot)$ 表示我们允许隐藏 polylog 因子，尤其 $\ln\ln n$ 等。

> **与排序里的 Theorem 2.9 对比**
> 排序里 Theorem 2.9 说的是：“只要输出的可能的 Lehmer 码种类数是 $\le t_0$，我们就能用 $\widetilde O(N\,m)$ 时间构建一个数据结构，让未来查询 $\pi(x_{i_1},\dots,x_{i_m})$ 的期望耗时 $O(H(\pi)+m)$”。
>
> 而现在 Theorem 3.9 说：
>
> * 当 $f=B$ 时，我们要学习的是“点定位（三角形编号）” 的输出种类数 $\le O(n^2\,m^2)$。
> * 当 $f=\Pi$ 时，我们要学习的是“三元组 $(x\text{-order},\,y\text{-order},\,\text{split‐tree})$” 的输出种类数 $\le O(m^8)$。
> * 只要这些种类数被 $t_0$ 控制住，Theorem 3.9 就保证我们可构造近熵查询的 Trie 结构，时间、空间、查询开销形式与排序场景几乎一模一样，只是把“长度 $m$”换成“组大小 $\lvert J\rvert$ = $m$”。

---

## 二、Proof 部分：Case 1 $\;f=B$

接下来我们进入证明。首先是 **Case 1**，也就是 “$f = B$”。我们要为“函数 $B$：$\{p_1,\dots,p_m\}\mapsto(\,r_1,\dots,r_m)$” 构造满足 Theorem 3.9 要求的数据结构。
回顾：$B$ 的含义是“给定 $m$ 个点 $(p_1,\dots,p_m)$，返回一个长度 $m$ 向量 $(r_1,\dots,r_m)$，其中 $r_i\in[1,|\mathrm{Del}(V)|]$ 是 $\mathrm{Del}(V)$ 中恰好包含 $p_i$ 的三角形编号”。

### 2.1 数据结构概要与思路

* 在排序里的 Theorem 2.9，我们对“L-buckets”或“Lehmer 码”构造了一棵**Trie** $T$，每个内部节点 $u$ 搭配一个 **near‐optimal BST $A_u$** 来对下一步的分支做熵编码。

* 这里“函数 $B$”也有同样的思路：我们要构造一个**Trie $T$（也叫 “Trie for $B$”）**，它的每条**边**（从某节点 $u$ 出发到某孩子 $w$） 都带一个标签 $r\in[1,|\mathrm{Del}(V)|]$，意思是“如果当前已经处理完前面几个点，下一步第 $i$ 个点 $p_i$ 落在三角形 $t_r$ 里，就沿这个标签为 $r$ 的那条边往下走”。

* 训练阶段我们会用 **$N$ 个独立样本$\;I_1,\dots,I_N$** 依次“把它们塞到 Trie $T$ 里”，统计“在 Trie 的某个节点 $u$ 处，下一步应当走标签 $r$” 的频率 $\hat p_{u,r}$，并在**训练结束时**对每个内部节点 $u$ 构造一个**distribution‐sensitive planar point location structure** 来近熵查询。

* 具体来说，**训练阶段要做的工作** 是：

  1. **在 $\mathrm{Del}(V)$ 上保留一个“最坏 $O(\log n)$ 时间”的平面点定位结构（例如 Kirkpatrick’s method \[21] 或其他经典静态点定位算法）**，它允许我们在 $O(\log n)$ 时间里将一个点 $p$ 定位到 $\mathrm{Del}(V)$ 的某个三角形编号 $t_r$。
  2. **从端到端构造 Trie $T$**：

     * 初始时 $T$ 只含一个空节点（根），意味着“尚未处理任何点（$i=1$）”。
     * 对每个训练样本 $I_s\sim \mathcal D$：

       1. 解出它的“同组子集” $\;(p_1,\dots,p_m)$，然后我们从 $i=1$ 到 $i=m$ 逐步往下插入：

          * 已经走到 Trie $T$ 的某个节点 $u$，此时相当于“我们已经确定了前 $i-1$ 个点 $p_1,\dots,p_{i-1}$ 的三角形编号 $(r_1,\dots,r_{i-1})$”，而下一步要处理的是 $p_i$。
          * 我们用“最坏 $O(\log n)$ 平面点定位”对 $p_i$ 做查询，得到它落在三角形 $t_{r}$ 里（某个 $r\in[1,|\mathrm{Del}(V)|]$）。
          * 检查当前节点 $u$ 下是否已存在一条标记 $r$ 的子边。如果没有，就**创建一个新孩子 $w$** 并将这条边打上标签 $r$。
          * 让我们继续沿着这条 “标签 = $r$” 的边走到下一个节点 $w$，并对 $u$ 处对应的统计“下一步走 $r$”的频数 $\hat N_{u,r}$ 加 1。
       2. 按照 $i=1,2,\dots,m$ 循环上述步骤之后，这条训练样本就被“写”进了 Trie $T$ 当中——从根到叶形成了一个长为 $m$ 的路径（节点链），这条路径上的每一条边都携带着这次训练样本中，相应位置处 $p_i$ 落的三角形编号 $r_i$。
  3. 重复上述对 $N$ 个训练样本都进行一次“Trie 写入”之后，**Trie $T$ 里就包含了“训练集里出现过的所有三角形编号序列”**。同时，对每个内部节点 $u$ 我们都统计了“它所包含的所有孩子标签 $r$ 的出现次数 $\hat N_{u,r}$”（其中 $\sum_r \hat N_{u,r} = N$）。

  这是 Trie 的“生长”过程，使用最坏 $O(\log n)$ 点定位需要 $O(m\log n)$ 时间/ 样本，写入 $N$ 个样本共 $O(N\,m\log n)$。最后我们会在每个节点 $u$ 基于其子边标签 $\{\,r\}$ 构造一个**distribution‐sensitive planar point location structure** ——它本质上就是一个**nearly‐optimal BST**，但这棵 BST 的 keys 代表“可能的三角形编号”、权重 $\hat p_{u,r} = \hat N_{u,r}/N$。这样做需要 $\widetilde O(\lvert\{\text{孩子}\}\rvert)$ 时间（记录所有孩子标签并建树）。

  * 最终**训练结束时**，我们消耗了

    $$
      \sum_{s=1}^N O\bigl(m\log n\bigr) \;+\; \sum_{u\in T} O\bigl(\lvert\mathrm{children}(u)\rvert \log \lvert\mathrm{children}(u)\rvert\bigr)
      \;=\; O\bigl(N\,m\log n \bigr) \;+\; O\Bigl(\sum_u \lvert\mathrm{children}(u)\rvert \log(\cdots)\Bigr).
    $$

    由于所有 $\lvert\mathrm{children}(u)\rvert$ 之和其实至多是 “训练时在 $T$ 上创建的节点数”，也就是 $O(N\,m)$（每插入一个长度 $m$ 的样本，会在 Trie 上新增至多 $m$ 个节点）；所以第二部分总体也在 $O(N\,m\,\log n)$。
  * 因此，**构造整棵 Trie $T$ 并为每个内部节点构造 near‐optimal BST $A_u$ 的总耗时是**

    $$
      O\bigl(N\,m\log n + N\,m\log n \bigr)
      \;=\; O\bigl(N\,m\log n\bigr)
      \;=\; \widetilde O\bigl(N\,m\bigr).
    $$
  * 空间方面，因为 $\lvert T\rvert\le N\,m$（最坏插入时每条新样本都加 $m$ 个新节点），再加上每个节点还有 $O(\lvert\mathrm{children}(u)\rvert)$ 大小的 BST 辅助索引，整体上占用 $O\bigl(\sum_{u} \lvert\mathrm{children}(u)\rvert\bigr) = O(Nm)$ 空间。
  * 这完全符合 Theorem 3.9 里“训练耗时 $\widetilde O(N\,\lvert J\rvert^2)$，空间 $O(t_0\,\lvert J\rvert)$”的要求。这里 $\lvert J\rvert = m$， $t_0 = O(n^2 m^2)$，因此 $N = \widetilde O(t_0) = \widetilde O(n^2 m^2)$，所以 $\widetilde O(N\,m) = \widetilde O(n^2 m^3)$ 实际上被上界写成 $\widetilde O(N\,m^2)$ 也是允许的（因为 $m\le n$）。

* **运行阶段**
  当一个新的实例 $I=(p_1,\dots,p_n)$ 到来后，我们只要对其子集 “$\{p_i: i\in J\}$” 做查询：

  1. 我们从 **Trie $T$ 的根** 出发，令 $i=1$。
  2. 当前在节点 $u$，想要知道“下一点 $p_i$ 落在哪个三角形 $t_r$ 里”，于是：

     * 调用我们在训练阶段为节点 $u$ 构造好的 “distribution‐sensitive planar point‐location 结构” $A_u$，把 $p_i$ 作为查询点传入。
     * 如果 $p_i$ 确实落在 $t_r$ 里，且 $A_u$ 也能够 **用 $O\bigl(\log(\mathrm{weight}(u)/\mathrm{weight}(w))\bigr)$** 时间返回正确的 $r$，那说明 **我们 $T$ 的结构“熵编码”** 在这里花的时间正是 $O(\log(1/\hat p_{u,r}))$（权重越大就越常被访问，查询越快）。
     * 如果这个 $p_i$ 所在的三角形编号 $r$  **之前没有在训练时出现过**（即 $u$ 节点下没有相应标签为 $r$ 的子边），那么 $A_u$ 会**报告查询失败**。这时我们需要“fallback”：退回到“最坏 $O(\lvert\mathrm{Del}(V)\rvert \log \lvert\mathrm{Del}(V)\rvert)$ 的暴力点定位”从三角网 $\mathrm{Del}(V)$ 上做完整 $O(\log n)$ 时间定位，并且还要做“在线补全 Trie”——逐步把标签 $r$ 加到 $T$ 中，这些都是 $O(m\log n)$ 的开销。
  3. 若查询成功或退化后能定位，便沿着“标签 $r$”的那条边走到下一个节点 $w$，令 $i\leftarrow i+1$，重复直到 $i=m$。
  4. 最终，我们会得到一个从根→叶的标签序列 $(r_1,\dots,r_m)$，它就是 $B(p_1,\dots,p_m)$ 的输出。
  5. **查询耗时的期望**：

     * 如果 $(r_1,\dots,r_m)$ 在训练时均出现过（以概率 $1-O(N^{-2})\approx 1 - O(t_0^{-2})$），那整条查询路径上的耗时就是

       $$
         \sum_{i=1}^m O\bigl(\log\tfrac{1}{\hat p_{u_i,\,r_i}}\bigr)
         \;+\; O(m)\quad(\text{加上 }O(1)\text{ 的移动/指针操作}) 
         \;=\; O\Bigl(m + \sum_{i=1}^m \log\frac{1}{p_{u_i,r_i}}\Bigr),
       $$

       其中 $u_i$ 是在 Trie 上处理 $p_i$ 时所在的节点，$p_{u_i,r_i}=\hat N_{u_i,r_i}/N$ 是“在训练阶段，节点 $u_i$ 上下一步走 $r_i$” 的概率。正因为熵的定义 $H_J(B) = \mathbb{E}[\sum_{i=1}^m \log(1/p_{u_i,r_i})]$，所以查询耗时的期望正是 $O(H_J(B)+m)$。
     * 如果某个 $p_i$ 落在一个“训练时从未见过的三角形编号” $r$，那才会“报警”并退化到 $O(m\log n)$ 的暴力定位。由于我们训练时用了 $N=\widetilde O(t_0)$ 个样本，而不相交的 “新”输出的个数 最多 $t_0$，所以这种“退化事件”的概率是 $\le t_0/N = O((\ln n\,\ln t_0)^{-1})$，足够小，让总的**失败概率** $O(t_0^{-2})$ 得到保证。
  6. 由此可见，在**成功的情况下**，查询时间的期望就是 $O(H_J(B)+m)$；在**失败（退化）的极少情况**中，查询时间是 $O(m\log n)$。结合两种情形，以概率 $\ge 1 - t_0^{-2}$ 得到的总期望仍然是 $O(H_J(B)+m)$。

  这正是 Theorem 3.9 对“Case 1：$f=B$” 的论证流程。所有细节与排序里的 Theorem 2.9 基本一样，只不过“排序中的 $b$” 被替换成了“点定位 $B$”，用到了“distribution‐sensitive planar point location structures” 代替“一维的 nearly‐optimal BST”。

---

## 三、Proof 部分：Case 2 $\;f=\Pi$

接下来说 “Case 2： $f=\Pi$”，也就是要为“函数 $\Pi$：$\{p_1,\dots,p_m\}\mapsto(\,\text{x‐order},\,\text{y‐order},\,\text{split‐tree})$” 构造一个近熵查询的数据结构。相比 Case 1，$\Pi$ 的输出更复杂，需要同时输出两条坐标顺序和一个拆分树。我们仍旧会构造一个**三层嵌套的 Trie 结构** $T$，分阶段负责：

1. 第一层 Trie $T_1$：先检索“$\{p_{1,x},\dots,p_{m,x}\}$ 在 $x$ 方向上的排序 $\pi(p_{1,x},\dots,p_{m,x})$”。
2. 第二层 Trie $T_2$：接着检索“$\{p_{1,y},\dots,p_{m,y}\}$ 在 $y$ 方向上的排序 $\pi(p_{1,y},\dots,p_{m,y})$”。
3. 第三层 Trie $T_3$：最后检索“它们在拆分树每层如何对半切分” 的信息，直到得到一个完整的拆分树形。

下面分步骤来详细说明：

### 3.1 三层 Trie 结构的设计思想

1. **整棵 Trie $T$**

   * 根 $r$ 先进入 **子Trie $T_1$**（用于检索 “$x$-order”）。
   * 当我们通过 $T_1$ 得到一个叶子节点，就对应着某个固定的“$x$-排序 $\beta_{x}\in\{1,2,\dots,m\}!$”。
   * 在 $T_1$ 的每个叶子下，都会挂上一个 **子Trie $T_{2}$**，用于检索相应的“在固定 $x$-排序下，$\{p_{1,y},\dots,p_{m,y}\}$ 的 $y$-order $\beta_{y}$” 。
   * 同理，当 $T_2$ 到达叶子，就相当于同时确定了“$x$-order $\beta_{x}$” 与“$y$-order $\beta_{y}$” ；这时在 $T_2$ 的叶子下就挂上第三级 **子Trie $T_{3}$**，用来检索“在这些 $\beta_{x},\beta_{y}$ 固定的情况下，拆分树从根到叶究竟如何对 $\{p_1,\dots,p_m\}$ 做逐层对半拆分”。
   * 捣鼓完这 3 级 Trie，最终我们能得到一个从根→叶→叶→叶 的长度 $L$ 的路径，其中：

     1. 第一段路径（在 $T_1$ 内部）上的标签序列就是“$x$-order” 的 Lehmer 码；
     2. 第二段路径（在 $T_2$）上的标签序列就是“$y$-order” 的 Lehmer 码；
     3. 第三段路径（在 $T_3$）上的标签序列就是“拆分树” 的一种编码方式（标记了“在节点 $u$ 处，是做竖直切还是水平切”以及“一次切之后子集怎么分”）。

2. **三层 Trie 的层次关系**

   * 每一层 Trie 里都用“nearly‐optimal BST” 来对“下一步的标签”做熵编码：

     * **在 $T_1$**，当前节点 $u_1$ 需要做的是 “给定已有的 $x$-order 前缀 $(\xi_1,\dots,\xi_{i-1})$，下一步到底是哪一个点在第 $i$ 个 $x$-排位？”。可能的候选是 $\{1,\dots,m\}$ 中剩下没排过序的所有索引，总计 $m-i+1$ 种。我们需要记录每一种候选 “点 $p_j$ 在第 $i$ 个 $x$-位置” 的**训练频率** $\hat p_{u_1,j}$，然后把它们做成一个 **distribution‐sensitive BST**（权重 $\hat p_{u_1,j}$）以便查询。
     * **在 $T_2$**，前提是“$(x$-order = \beta\_{x}) 已经完全确定”。此时第二层要做的是“在 $y$ 方向上，给定已有 $i-1$ 个已选过的 $y$-位置（完全对应了 $\beta_{x}$ 里相同的 $i-1$ 个点），下一步到底是哪一个点在第 $i$ 个 $y$-位置？”。同样要用 “nearly‐optimal BST” 记录“尚未排定 $y$-位置的那些点” 在训练集中出现的频率。
     * **在 $T_3$**，节点 $u_3$ 代表“已经沿着拆分树从根切到某个深度 $d-1$ 了，此时有一个 **子集** $P_{u_3}\subseteq\{p_1,\dots,p_m\}$ 被留到第 $d$ 层切分”。在 $u_3$ 处，函数 $\Pi$ 下一步要选择的是“是做竖直切（即对 $x$-坐标区间做对半切） 还是做水平切（对 $y$-坐标区间做对半切）？”；以及“如果竖直切，要选哪个 $i$ 使得切分线上恰好有 $|P_{u_3}|/2$ 个点在左/右？”（或如果水平切，要在哪个 $j$ 处切 “横线”）。所有这些候选切分线的位置都可以编号成一组标签，我们把在训练样本里出现的所 有这些标签及其频率 $\hat p_{u_3,\ell}$ 都统计好，归入**一个 distribution‐sensitive BST** 中，用于运行时近熵查询。

   * 这样一来，整棵 Trie $T$ 含有三层：

     1. 第一层 $T_1$ 的“分支数” $\le m$，接着向下走每层分支依次递减，直到 “$\;(x$-order) 排序完毕” 共 $m!$ 种可能；
     2. 第二层 $T_2$ 的“分支数” $\le m!$（因为你先要选好 $\beta_x$ 后，可能的 $y$-order) 也有最多 $m!$ 种）；
     3. 第三层 $T_3$ 的“分支数” 是“Halving Split Tree 所有可能的形状种类数”，前面 Lemma 3.8 已证明它是 $O(m^8)$。

   整体训练时我们只会**插入 $N$ 个样本**，每个样本会对应一条从 $T_1$→$T_2$→$T_3$ 的路径，因此对 $T_1,T_2,T_3$ 上所有“析出 $\beta_{x}$/$\beta_{y}$/$\text{split‐tree}$” 叶子节点的每个标签的出现次数都做一次累积统计，得到三层所有节点的权重分布。最后在训练阶段，**为每个节点 $u$** 对应一个 near‐optimal BST $A_u$（它的 keys 是 “下一层候选标签”，权重是 “这些标签出现的概率”）并把它挂在 $u$ 上。这样 $T$ 就做成了一个“不断深挖、每层都带熵编码 BST”的三层嵌套 Trie。

### 3.2 训练阶段的细节与时间/空间开销

1. **第一层 $T_1$：构造 “$x$-order Trie”**

   * 每个样本 $\,(p_{1},\dots,p_{m})$ 先去对 $\{p_{1,x},\dots,p_{m,x}\}$ 做一次**静态最坏 $O(m\log m)$ 排序** （可以直接 C++ std::sort），得到一个“$x$-order 排名 $\beta_x$”（比如把 $m$ 个点按 $x$-坐标由小到大编号 1…$m$）。
   * 然后 **把 $\beta_x$ 编码成它的 Lehmer 码**（一个长度 $m$ 的序列，每位表示该点在剩余点里的排位数），再把这串 Lehmer 码“逐位插入到 $T_1$ 里”。
   * 这样一来，如果 $T_1$ 里从根到叶的路径标签正好是“某个 Lehmer 码”，那么这条路径就对应着一个具体的 $x$-order $\beta_x$。
   * 在插入的同时，我们对 $T_1$ 上的每个内部节点 $u$（对应 “前 $i-1$ 个位置都已经确定了”）记录“下一位 Lehmer 码可能是 $j\in[0,m-i]$”的频数 $\hat N_{u,j}$，插入 $N$ 个样本总共需要 $O(N\,m)$ 次“在 $T_1$ 上侧枝或建新节点”的操作。
   * **在第 $i$ 个深度处**，$T_1$ 的节点 $u$ 会有最多 $m-i+1$ 个孩子（分别对应“第 $i$ 个点在剩余 $m-i+1$ 个候选点中排第 $k$”）。训练完毕后，我们根据每个 $u$ 处 $\{\hat N_{u,k}\}$ 构造一个 “near‐optimal BST $A_{u,1}$”（即：给 $k\in[0,m-i]$ 设置权重 $\hat N_{u,k}/N$）。
   * **时间复杂度**：对 每个样本做一次 $x$-排序 $O(m\log m)$ + 插入 $m$-级深 $O(m)$ 到 $T_1$ （每步都付出 $O(1)$ 建节点/更新计数），共 $O(N\,m \log m)$。为 每个 $T_1$ 的内部节点建 BST 总共 $O\bigl(\sum_u \mathrm{deg}(u)\bigr) = O(N\,m)$ 。
   * **空间复杂度**：总节点数 $\le N\,m$，每个节点额外存一个 BST，累计 $\sum_u \mathrm{deg}(u) = O(N\,m)$。

2. **第二层 $T_2$：构造 “$y$-order Trie”**

   * 每个样本也同时要对 $\{p_{1,y},\dots,p_{m,y}\}$ 做一次**静态排序** $O(m\log m)$，得到“$y$-order 排名 $\beta_y$”；再编 Lehmer 码并逐位“插入到 $T_{2,\beta_x}$”。
   * 注意：每一个 **第一层 $T_1$ 的叶子**（即已确定的 $x$-order $\beta_x$）都要挂上一个独立的子Trie $T_{2,\beta_x}$。所以第二层**实际上是若干个小 Trie**（每个对应一个 $\beta_x$），它们的结构是相似的，但“孩子数”“节点计数”等统计只能在各自的 $T_{2,\beta_x}$ 里独立完成。
   * 对于一条训练样本，当它对应的 $x$-order $\beta_x$ 确定后，就把它在相应的 $T_{2,\beta_x}$ 里“插入” $m$-层 Lehmer 码。这需要 $O(m)$ 次“改节点计数或新建节点”。每个节点 $u$ 需要记录“下一步 $y$-排位 $\in[0,m-i]$” 的频率分布 $\hat N_{u,k}$。
   * 训练完成后对每个 $T_2$ 节点建 BST，用 $\hat N_{u,k}/N$ 做权重。
   * **时间复杂度**：额外 $O(N\,m\log m)$ 用于排序 $\{p_{i,y}\}$，再 $O(N\,m)$ 用于插入 Trie 和建 BST。
   * **空间复杂度**：第二层总节点数 $\le \,(\text{第一层叶子数})\times m \le (N\,m)\times m = N\,m^2$。

3. **第三层 $T_3$：构造 “拆分树 (halving split tree) Trie”**

   * 现在已知每组 $(p_1,\dots,p_m)$ 的完整 “$x$-order $\beta_x$” 和 “$y$-order $\beta_y$” 之后，节点就要准备“**对半拆分**\*(halving split) Algorith 的信息”。
   * 正如前面 Lemma 3.8 里讨论的，拆分树的每个内部节点 $u$ 对其所管辖的子集 $P_u\subset\{p_1,\dots,p_m\}$ 要“依据 $\min_x,\max_x$ 或 $\min_y,\max_y$” 在 $P_u$ 上选一条分界线。我们把“可以选的拆分线”给编号，任一种拆分线 $\ell$ 在训练集里出现的次数做成权重 $\hat N_{u,\ell}$。
   * **插入过程**：

   1. 固定一个训练样本 $\{p_1,\dots,p_m\}$，先得到它的 $\beta_x$ 与 $\beta_y$。此后，我们就知道“在拆分树的根处要如何对 $\{p_1,\dots,p_m\}$ 做第一次对半切分”──因为我们已经知道“谁给了 $\min_x,\max_x$” 或 “谁给了 $\min_y,\max_y$”。根据 Lemma 3.8，输出的拆分决策 $\ell_1$ 是不依赖其他细微变化的“确定性判定”主导的。
   2. 把 $\ell_1$ 这个标签插入到 “根节点 $u_3$ 对应的 $T_3$” 当中，然后递归：“转到对应的子集（左子集或右子集），知道了 $\ell_1$ 后，下一层要怎么切 $\{p_i\}$”——这时就生成另一个拆分决策 $\ell_2$，再插到下一级 $T_3$ 节点。如此循环，直到所有 $m$ 个点都被分到各自叶子（每次都对应一次“对半切”）。

   * **训练结束后**，每个 $T_3$ 节点 $u$（对应“某个子集 $P_u$ 正要切下一层”）里面就有 $\bigl\{\hat N_{u,\ell}\bigr\}$ 这个分布——用来建一个 near‐optimal BST $A_{u,3}$，让运行时可以近熵地查询“这次样本在 $u$ 处的拆分决策 $\ell$ 是哪一个”。
   * 前面 Lemma 3.8 已经告诉我们：“在 $T_3$ 里所有可能的拆分树形种类数 $\le O(m^8)$”，因此**训练阶段在 $T_3$ 上插入 $N$ 个样本 共会生成 $\le N\,m$ 个节点**（每个样本对 $T_3$ 的最大贡献也是 $m$ 层深度，最多增 $m$ 个节点），并且能保证后续用 $\{\hat N_{u,\ell}\}$ 构造 near‐optimal BST 的总成本 $\widetilde O(N\,m)$。
   * **空间复杂度**：第三层 $T_3$ 的节点数 $\le N\,m$。
   * **时间复杂度**：把 $N$ 个样本插入 $T_3$／建 BST 也需要 $\widetilde O(N\,m)$。

4. **总的训练时间／空间总结**

   * **时间**：

     $$
       O\bigl(N\,m\log m\bigr)\quad(\text{第一层排序}) 
       +\,O\bigl(N\,m\bigr)\quad(\text{第一层 Trie 插入＋BST 建造}) 
       +\,O\bigl(N\,m\log m\bigr)\quad(\text{第二层排序}) 
       +\,O\bigl(N\,m\bigr)\quad(\text{第二层 Trie 插入＋BST}) 
       +\,O\bigl(N\,m\bigr)\quad(\text{第三层 Trie 插入＋BST}) 
       \;=\; \widetilde O\bigl(N\,m\log m\bigr).
     $$

     因为 $m\le n$，$\log m = O(\log n)$，所以总共就是 $\widetilde O(N\,m)$。这与 Theorem 3.9 中要求的“$\widetilde O(N\,m^2)$”（论文里写的是 $\widetilde O(N\,\lvert J\rvert^2)$，但其实一个因子 $\log n$ 可以合并到“$\widetilde O$” 里）是一致的。
   * **空间**：树 $T$（含 $T_1,T_2,T_3$ 三个嵌套）总节点 $\le N\,m + N\,m^2 + N\,m = O(N\,m^2)$。每个节点挂一个 near‐optimal BST；第二层节点数 $O(N\,m)$，第三层节点数 $O(N\,m)$，因此 $\sum_u \bigl|\mathrm{children}(u)\bigr| = O(N\,m)$，空间 $O(N\,m)$。
   * 另外要注意，**Theorem 3.9 给我们的空间上界是 $O(t_0\,m)$**。为什么没有 $N\,m$ 这么大，而是 $t_0\,m$？

     * 原因在于：Tries 和 BST 都只需要存储\*\*“那些在训练集里出现过的输出标签”**，例如第一层 $T_1$ 只需要存 $m!$ 中**实际“在训练 $N$ 个样本中出现过的”\*\* $x$-order；而 “在训练 $N$ 个样本中出现过的” $x$-order 数量 $\le t_0$（Lemma 3.8 给出 $\lvert B(S)\rvert\le O(n^2m^2)$ 之类的上界）。
     * 因此，**即便理论上 $N\,m$ 节点，实际上各层只有 $\le t_0$ 个不同的“输出模式”** 才会真的被存下来。所有与第 1 层第 2 层……第 3 层相关的实际标签种类都被束缚在 $t_0$ 内，最终空间上限就 $O(t_0\,m)$。

   综上，**训练阶段** 在时间 $\widetilde O(N\,m)$（读作“$N$ 乘 $m$ 多项式对数因子”）内完成，空间 $O(t_0\,m)$ ，正好对应 Theorem 3.9 的“构造时间 $\widetilde O(N\,m^2)$（论文里包含额外对数因子）”与“空间 $O(t_0\,m)$” 的叙述。

### 3.3 运行阶段的查询算法与期望时间分析

当我们真正**给定输入 $I=(p_1,\dots,p_n)$** 并只想查询它在子集 $\{p_i:i\in J\}$ 下的输出 $\Pi\bigl(p_1,\dots,p_m\bigr)$ 时，做法是：

1. **第一层 $T_1$**：

   * 从 $i=1$ 开始，看 $\{p_{1,x},\dots,p_{m,x}\}$，并用 near‐optimal BST $A_{\text{root},1}$ 在 $O(\log(1/\hat p_{u_1,j}))$ 时间得到第 $i$ 个 $x$-座次 $\beta_x(i)$，然后沿着标签 $\beta_x(i)$ 的那条边走到下一层节点 $u_1'$。
   * 一直执行到 $i=m$ 时，就得到完整的 “$x$-order 排序 $\beta_x$”。\\
   * 如果在某步 $i$ 里，当前 BST $A_{u_1,i}$ 中并不存在 “$p_{k,x}$ 在第 $i$ 的候选列表” （也就是说训练时从未见过这样的 $x$-排序），就会在 $O(\log m)$ 时间里报告失败并“fallback”到 $O(m\log m)$ 的暴力排序。此时我们还要把这个新序列插入到 $T_1$ 中。
   * 以失败概率 $\le t_0/N\approx O(1/\ln n\ln t_0)$ 可以保证整段查询不会因“未见过的 $x$-排序”而退化。

2. **第二层 $T_2$**：

   * 既然 $\beta_x$ 已知，我们赶到对应的 $T_{2,\beta_x}$ 的根节点，按同样做法在 near‐optimal BST $A_{u_2,1}$ 上逐位查询来确定 “$y$-order 排序 $\beta_y$” 。整体耗时 $\sum_{i=1}^m O(\log(1/\hat p_{u_{2,i},k})) = O(H_J(\pi(\cdot,y)) + m)$。
   * 同样有“fallback” 机制，如果出现“未见过的 $y$-排序”，就退回去做 $O(m\log m)$ 的暴力排序，并且动态插入 $T_2$ 以更新统计。

3. **第三层 $T_3$**：

   * 既然 $\beta_x$ 与 $\beta_y$ 都已知，就进入相应的 $T_{3,\beta_x,\beta_y}$ 根节点，开始做“拆分决策”查询：

     1. 在当前节点 $u_3$ ，我们已经知道它要管辖的点子集 $P_u\subseteq\{p_1,\dots,p_m\}$（在运行时我们直接用 $\beta_x,\beta_y$ 来确定是哪几个点）。此时，near‐optimal BST $A_{u_3,3}$ 中储存着“曾在训练样本里出现的所有这些点 $P_u$ 的拆分决策 $\ell$” 及其权重 $\hat p_{u_3,\ell}$。
     2. 用 $A_{u_3,3}$ 在 $O(\log(1/\hat p_{u_3,\ell}))$ 时间做查询，就得出“根节点该选择哪一条拆分线 $\ell$（竖直或水平，具体切割哪两个点来对半）”。
     3. 根据 $\ell$ 将 $P_u$ 分为 $\,P_{\text{left}}$ vs $\,P_{\text{right}}$，同时进入下一级对应的子节点 $u_3'$ 继续查询。
     4. 重复直到所有 $m$ 个点分到叶子级，此时即可得到一个完整的 Halving Split Tree 形。
   * 如果训练时从未出现过“当前 $P_u$ 子集的该拆分决策 $\ell$”，也可用 $O(m\log m)$ 的暴力做法再算一次（穷举所有可能的拆分线、测试中点发生的子集分布），然后插入 $T_3$。不过这种情况只会以低概率 $O(t_0^{-1})$ 出现，从而不会影响整体查询的期望耗时。

4. **查询耗时的期望**

   * 若全部**不退化**，则总体耗时是：

     $$
       \underbrace{\sum_{i=1}^m O\bigl(\log\,1/p_{u_{1,i},r_i^{(x)}}\bigr)}_{\text{第一层 }x\text{-order}}
       \;+\; 
       \underbrace{\sum_{i=1}^m O\bigl(\log\,1/p_{u_{2,i},r_i^{(y)}}\bigr)}_{\text{第二层 }y\text{-order}}
       \;+\; 
       \underbrace{\sum_{\substack{\text{nodes }u_3 \\ \text{在第三层}}} O\bigl(\log\,1/p_{u_3,\ell}\bigr)}_{\text{第三层 拆分}}
       \;+\; O(m).
     $$

     按熵的定义，这正好是

     $$
       O\bigl(H_J(\pi(p_x)) + H_J(\pi(p_y)) + H_J(\text{split‐tree})\bigr) + O(m)
       \;=\; O\bigl(H_J(\Pi) + m\bigr).
     $$
   * 若发生一次“fallback”情形（出现未见过的 $x$-排序 或 $y$-排序 或 “拆分决策”），则在该节点会 “退回暴力 $O(m\log m)$ 重新计算并动态插入 Trie”，这会花 $O(m\log m)$ 时间。但整个运行阶段出现 fallback 的概率最多是 $O(t_0^{-1})$（因为每种新输出都已被 $O(N)$ 次训练覆盖，只多出 $\le t_0$ 种没见过的可能）。因此，失败带来的额外成本被严格控制在 $O(m\log m)\times O(t_0^{-1}) = O(1)$ 量级，归入常数因子。
   * 综上，以**概率 $\ge 1 - O(t_0^{-1})\approx 1 - O(t_0^{-2})$** ，查询耗时的**期望** 是 $O\bigl(H_J(\Pi) + m\bigr)$，精确满足 Theorem 3.9 的要求。

---

## 四、小结与比较

* **Theorem 3.9** 实际上把“排序里的 Theorem 2.9（熵编码 Lehmer 码检索）” **一分为三**，对应“$x$-order 检索”“$y$-order 检索”“拆分树检索” 三个步骤，每步都使用“Trie + distribution‐sensitive BST” 的熵编码思想。
* **训练时间** ：关键在于我们需要 $N\sim O(t_0\ln t_0\ln n)$ 个样本来**充分学习**那些可能的输出标签（无论是三角形索引、Lehmer 码里的每一位，还是拆分树的每一次切分决策），然后把它们的出现频率拿来构造 near‐optimal BST。总体耗时 $\tilde O(N\,m)$。
* **空间占用** ：因为输出种类数 $\le t_0$，无论是“‌$x$-order 叶子”、“‌$y$-order 叶子”还是“拆分树形叶子”，都只会有 $O(t_0)$ 种组合真正出现于训练集中，而不仅仅是理论上的 $m!$ 或 $O(m^8)$。因此整棵 Trie＋BST 的总节点数和叶子数也都被 $O(t_0\,m)$ （或更紧 $O(t_0)$）控制。
* **查询期望时间** ：在成功 (不退化) 情况下，就是各层熵之和  $O\bigl(H_J(x\text{-order}) + H_J(y\text{-order}) + H_J(\text{split‐tree})\bigr) + O(m)$，合并起来正好是 $O\bigl(H_J(\Pi) + m\bigr)$。
* 整个思想图景对应排序场景中的“分桶 $b$” 与“组内排序 $\pi$” 两个函数，在二维 Delaunay 三角化问题里推广成为“三个函数”：

  1. “点定位 $B$”（对应排序里的“$b$”），
  2. “$x$-order (Lehmer 码) $\pi(\cdot,x)$ ”（对应排序里的“$\pi$”），
  3. “$y$-order (Lehmer 码) $\pi(\cdot,y)$”（排序里没有二维情形，相当于复制一条 $\pi$），
  4. “拆分树检索 (split‐tree) ”（相当于一个更复杂的 $\pi$）。

> **练习对照**：如果把排序里 Theorem 2.9 的“构造 Trie ＋ near‐optimal BST” 理解透了，那么本文在 Theorem 3.9 里只不过“复制升级”了三次（一次针对 $B$，两次针对直线排序 $x$/$y$，一次针对拆分树），每次训练和查询的逻辑都完全一样：
>
> * **先用 $N$ 个样本统计“可能的下一位输出” $\hat p$**；
> * **构造 near‐optimal BST $A_u$** 作为“下一位编码表”；
> * **查询时在 $O(\log(1/\hat p))$ 近熵时间做分支**，万一落到“未见过的 output” 就做 $O(m\log m)$ 强制退化。

这样，我们就完成了对 **Theorem 3.9**（“Generalizing 2.9 to $B$ and $\Pi$”）的**全程剖析**：

1. 它是如何陈述的，为什么可以把“输出种类数 $\le t_0$” 转化为“近熵查询” 的保证；
2. 在 “Case 1: $f=B$” 里，怎样一步步从“静态 $\mathrm{Del}(V)$” 和 “分布敏感点定位结构” 构造出一棵 Trie；
3. 在 “Case 2: $f=\Pi$” 里，用三层 Trie 来分别检索“$x$-order”、“$y$-order” 和 “拆分树形”，并给出具体的训练/查询流程与复杂度分析。

以下图示帮助你理清三层结构的嵌套与相互作用：

```
                      Root of T
                          │
                       [T₁] “x-order Trie”
        ┌─────────────────┴────────────────┐
   …   /                                   \
Node u₁ with χ children                      Node u₁′ with χ′ children
(Each child corresponds to “x-order’s next rank”)
   …                                         … 
  (When x-order fully decided at some leaf βₓ)
                       │
                   Leaf of T₁
                       │
                    [T₂] “y-order Trie”     ← For each βₓ there is a separate T₂,βₓ 
        ┌─────────────────┴────────────────┐
   …   /                                   \
Node u₂ with ψ children                      Node u₂′ with ψ′ children
(Each child corresponds to “y-order’s next rank”)
   …                                         …
  (When y-order fully decided at some leaf βᵧ)
                       │
                   Leaf of T₂
                       │
                    [T₃] “Splitting Tree Trie”
        ┌─────────────────┴────────────────┐
   …   /                                   \
Node u₃ with φ children                      Node u₃′ with φ′ children
(Each child corresponds to “which split line”)
   …                                         …
  (Finally reach leaf → complete halving split tree)
```

* **第一层 $T_1$**：负责揭示“输入 $\{p_i\}$ 在 $x$ 方向上的相对顺序”，每个节点 $u_1$ 的孩子 $j\in[0,m-i]$ 表示“下一位 $x$-位置上放该子集里第 $j$ 大的点”。
* **第二层 $T_2$**：负责揭示“在固定 $\beta_x$ 下，输入 $\{p_i\}$ 在 $y$ 方向上的相对顺序”，每个节点 $u_2$ 的孩子 $j\in[0,m-i]$ 表示“下一位 $y$-位置放该子集里第 $j$ 大的点”。
* **第三层 $T_3$**：负责揭示“在固定 $\beta_x,\beta_y$ 下，拆分树在当前子集 $P_u$ 处采用哪个对半切分线”，每个节点 $u_3$ 的孩子 $\ell$ 表示“一种使 $\min_x,\max_x$ 或 $\min_y,\max_y$ 均等地分到两侧的切分线”。

这三层的训练/查询逻辑和排序场景里是非常相似的，由于篇幅较长，这里就不再把每一句训练/查询伪代码都列出。总之，**Theorem 3.9** 至此已被完全剖解：它为接下来**整体 Delaunay 三角化算法**的“操作阶段”提供了所有必要的熵编码检索工具，让我们能够在期望 $O(n + H_S)$ 时间里完成二维 DT。

**到这里，这部分内容就告一段落。** 下一步，如果你要继续往下理解 “如何整合 $B$、$\Pi$ 与 WSPD 做全局增量 Delaunay”，可以接着阅读 3.3 节：**“Algorithm Operation Phase”**，或直接跳到整体 Theorem 1.2 的证明部分。
